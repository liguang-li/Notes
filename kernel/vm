Descibing physical memory

NUMA: non-uniform memory access

With large scale machines, memory may be arranged into banks
that incur a different cost to access depending on the “distance”
from the processor. For example, there might be a bank of memory
assigned to each CPU or a bank of memory very suitable for DMA
near device cards.

Each bank is called a node and the concept is represented under
Linux by a struct pglist_data even if the architecture is UMA.
This struct is always referenced to by it's typedef pg_data_t.

Every node in the system is kept on a NULL terminated list called
pgdat_list and each node is linked to the next with the field 
pg_data_t->node_next. For UMA architectures like PC desktops,
only one static pg_data_t structure called contig_page_data is used. 

Each node is divided up into a number of blocks called zones which
represent ranges within memory. 

Zones should not be confused with zone based allocators as they are
unrelated. 

A zone is described by a struct zone_struct, typedeffed to zone_t
and each one is of type ZONE_DMA, ZONE_NORMAL or ZONE_HIGHMEM.

Each zone type suitable a different type of usage. 
ZONE_DMA is memory in the lower physical memory ranges which certain 
ISA devices require.
Memory within ZONE_NORMAL is directly mapped by the kernel into the 
upper region of the linear address space.
ZONE_HIGHMEM is the remaining available memory in the system and is
not directly mapped by the kernel.

With the x86 the zones are:

ZONE_DMA	First 16MiB of memory
ZONE_NORMAL	16MiB - 896MiB
ZONE_HIGHMEM	896 MiB - End

Each physical page frame is represented by a struct page and all the
structs are kept in a global mem_map array which is usually stored at
the beginning of ZONE_NORMAL or just after the area reserved for the
loaded kernel image in low memory machines.

			pg_data_t
			    |
			node_zones
			    |
	ZONE_DMA	ZONE_NORMAL	ZONE_HIGHMEM
	    |		    |		     |
	node_mem_map	node_mem_map	   node_mem_map
	    |		    |		     |
	   page		   page		    page


Nodes:
Each node in memory is described by a pg_data_t which is a typedef for
a struct pglist_data. When allocating a page, Linux uses a node-local 
allocation policy to allocate memory from the node closest to the running
CPU. As processes tend to run on the same CPU, it is likely the memory 
from the current node will be used. 

The struct is declared as follows in <linux/mmzone.h>

129 typedef struct pglist_data {
130     zone_t node_zones[MAX_NR_ZONES];		//The zones for this node, ZONE_HIGHMEM, ZONE_NORMAL, ZONE_DMA
131     zonelist_t node_zonelists[GFP_ZONEMASK+1];	//his is the order of zones that allocations are preferred from. 
							//A failed allocation in ZONE_HIGHMEM may fall back to ZONE_NORMAL or back to ZONE_DMA;
132     int nr_zones;					//Number of zones in this node, between 1 and 3
133     struct page *node_mem_map;			//
134     unsigned long *valid_addr_bitmap;		//A bitmap which describes “holes” in the memory node that no memory exists for. (Sparc and Sparc64)
135     struct bootmem_data *bdata;			//This is only of interest to the boot memory allocator
136     unsigned long node_start_paddr;/node_start_pfn	//The starting physical address of the node
137     unsigned long node_start_mapnr;			//gives the page offset within the global mem_map ?
138     unsigned long node_size;/node_present_pages and node_spanned_pages	//The total number of pages in this zone
139     int node_id;					//The Node ID (NID) of the node, starts at 0
140     struct pglist_data *node_next;			//Pointer to next node in a NULL terminated list
141 } pg_data_t; 

All nodes in the system are maintained on a list called pgdat_list. 

Initialized by the init_bootmem_core() function.
Macro for_each_online_pgdat() is used to traverse the list


Zones
Zones are described by a struct zone_struct and is usually referred
to by it's typedef zone_t. It keeps track of information like page 
usage statistics, free area information and locks.

It is declared as follows in <linux/mmzone.h>:

37 typedef struct zone_struct {
41     spinlock_t        lock;				//Spinlock to protect the zone from concurrent accesses
42     unsigned long     free_pages;			//Total number of free pages in the zone
43     unsigned long     pages_min, pages_low, pages_high; //zone watermarks
44     int               need_balance;			//tells the pageout kswapd to balance the zone
45 
49     free_area_t       free_area[MAX_ORDER];		//Free area bitmaps used by the buddy allocator
50 
76     wait_queue_head_t * wait_table;			//A hash table of wait queues of processes waiting on a page to be freed
77     unsigned long     wait_table_size;		//Number of queues in the hash table which is a power of 2
78     unsigned long     wait_table_shift;		//the number of bits in a long minus the binary logarithm of the table size above ??
79 
83     struct pglist_data *zone_pgdat;			//Points to the parent pg_data_t
84     struct page        *zone_mem_map;		//The first page in the global mem_map this zone refers to
85     unsigned long      zone_start_paddr;
86     unsigned long      zone_start_mapnr;
87 
91     char               *name;			//The string name of the zone, “DMA”, “Normal” or “HighMem”
92     unsigned long      size;				//The size of the zone in pages
93 } zone_t;

Zone Watermarks
When available memory in the system is low, the pageout daemon 
kswapd is woken up to start freeing pages

Each zone has three watermarks called pages_low, pages_min and 
pages_high which help track how much pressure a zone is under.

PAGES_LOW When pages_low number of free pages is reached, kswapd 
is woken up by the buddy allocator to start freeing pages. 
The value is twice the value of pages_min by default;

PAGES_MIN When pages_min is reached, the allocator will do the 
kswapd work in a synchronous fashion, sometimes referred to as 
the direct-reclaim path. 

PAGES_HIGH Once kswapd has been woken to start freeing pages it 
will not consider the zone to be “balanced” when pages_high pages 
are free. Once the watermark has been reached, kswapd will go back 
to sleep. 
The default for pages_high is three times the value of pages_min.


Calculating the Size of Zones
			setup_memory

find_max_pfn find_max_low_pfn	init_bootmem  register_bootmem_low_pages	find_smp_config

				init_bootmem_core	free_bootmem	  	find_intel_smp
							free_bootmem_cores smp_	scan_config
  									reserve_bootmem 
									reserve_bootmem_core

The pfn is an offset, counted in pages, within the physical memory map.
The first PFN usable by the system, min_low_pfn is located at the beginning
of the first page after _end which is the end of the loaded kernel image.
The value is stored as a file scope variable in mm/bootmem.c for use with
the boot memory allocator.

Zone Wait Queue Table
When IO is beginning performed on a page, such are during page-in or page-out,
it's locked to prevent accessing it with inconsistent data. Processor wishing
to use it have to join a wait queue before it can be accessed by calling
wait_on_page(). When the IO is completed, the page will be unlocked with
UnlockPage() and any process waiting on the queue will be woken up. Each page
could have a wait queue but it would be very expensive in terms of memory to 
have so many separate queues so instead, the wait queue is stored in zone_t.


Process A wants to lock page

__wait_on_page 		--->  page_waitqueue --->	struct page
							page->flags
			add_wait_queue()
Sleeping 						page_zone()
							wait_queue_head_t
							wait_queue_head_t
							wait_queue_head_t
							zone->wait_table

Zone Initialization
The zones are initialised after the kernel page tables have been fully setup by paging_init(). 

nid is the Node ID which is the logical identifier of the node whose zones are being initialised;
pgdat is the node's pg_data_t that is being initialised. In UMA, this will simply be contig_page_data;
pmap is set later by free_area_init_core() to point to the beginning of the local lmem_map array allocated for the node.
zones_sizes is an array containing the size of each zone in pages
zone_start_paddr is the starting physical address for the first zone;
zone_holes is an array containing the total size of memory holes in the zones


Inintializing mem_map
The mem_map area is created during system startup in one of two fashions.
On NUMA systems, the global mem_map is treated as a virtual array starting at PAGE_OFFSET.
free_area_init_node() is called for each active node in the system which allocates 
the portion of this array for the node being initialised.

On UMA systems, free_area_init() is uses contig_page_data as the node and the 
global mem_map as the “local” mem_map for this node. 


PAGES
Every physical page frame in the system has an associated struct page which is 
used to keep track of its status.

152 typedef struct page {
153     struct list_head list;		//Pages may belong to many lists and this field is used as the list head.
154     struct address_space *mapping;	//When files or devices are memory mapped, their inode has an associated address_space.
155     unsigned long index;		//This field has two uses and it depends on the state of the page what it means.
					//If the page is part of a file mapping, it is the offset within the file. If the page is 
					//part of the swap cache this will be the offset within the address_space for the swap address space
156     struct page *next_hash;		//Pages that are part of a file mapping are hashed on the inode and offset. 
158     atomic_t count;			//The reference count to the page
159     unsigned long flags;		// flags which describe the status of the page.
161     struct list_head lru;		//For the page replacement policy, pages that may be swapped out will exist on 
					//either the active_list or the inactive_list
163     struct page **pprev_hash;	//
164     struct buffer_head * buffers;	//If a page has buffers for a block device associated with it, this field is 
					//used to keep track of the buffer_head
175
176 #if defined(CONFIG_HIGHMEM) || defined(WANT_PAGE_VIRTUAL)
177     void *virtual;
179 #endif /* CONFIG_HIGMEM || WANT_PAGE_VIRTUAL */
180 } mem_map_t;


Mapping Pages to Zones
set_page_zone(page, nid * MAX_NR_ZONES + j);


High Memory
As the addresses space usable by the kernel (ZONE_NORMAL) is limited in size, the 
kernel has support for the concept of High Memory. 

To access memory between the range of 1GiB and 4GiB, the kernel temporarily maps 
pages from high memory into ZONE_NORMAL with kmap().



Page Table Management
Describing the Page Directory
Each process a pointer (mm_struct->pgd) to its own Page Global Directory (PGD) which
is a physical page frame. This frame contains an array of type pgd_t which is an 
architecture specific type defined in <asm/page.h>.

On the x86, the process page table is loaded by copying mm_struct->pgd into the cr3 
register which has the side effect of flushing the TLB. In fact this is how the function
__flush_tlb() is implemented in the architecture dependent code.

Each active entry in the PGD table points to a page frame containing an array of Page
Middle Directory (PMD) entries of type pmd_t which in turn points to page frames containing
Page Table Entries (PTE) of type pte_t, which finally points to page frames containing the
actual user data. 

In the event the page has been swapped out to backing storage, the swap entry is
stored in the PTE and used by do_swap_page() during page fault to find the swap 
entry containing the page data. 

				linear address
offset within process PGD	offset within PMD page frame	offset within PTE page frame	offset within Date frame
	|				|				|				|	________
	|				|				|				|	|	|
	|				|				|				------->|	|
	|				|				|		________		|	|
	|				|				|		|	|		|	|
	|				|				pte_offset()--->|pte_t	|-------------->|_______|
	|				|						|	|		Page frame
	|				|		 ________			|	|		with user date
	|				|		 |	|			|	|
	|				pmd_offset()---->|pmd_t |---------------------->|_______|
	|						 |	|			pte_t page
	|     	       _______				 |	|			frame
	|	       |      |				 |	|
pgd_offset()---------->|pgd_t |------------------------->|______|
  		|      |      |				pmd_t page
		|      |      |				frame
	    pdg_index()|      |
		|      |      |
mm_struct->pgd--------->______|
			only 1 
			pgd_t page frame


Any given linear address may be broken up into parts to yield offsets
within these three page table levels and an offset within the actual page.

The SHIFT macros specifies the length in bits that are mapped by each level of the page tables.

					linear address
					BITS_PER_LONG
		________________________________________________________________
		|		|		|		|		|
		|PGD		|PMD		|PTE		|OFFSET		|
		-----------------------------------------------------------------
								<--PAGE_SHIFT--->
						<-----------PMD_SHIFT----------->
				<--------------------PGDIR_SHHIFT--------------->


The MASK values can be AND with a linear address to mask out all the
upper bits and is frequently used to determine if a linear address
is aligned to a given level within the page table.

The SIZE macros reveal how many bytes are addressed by each entry at each level.

					Linear address
					BITS_PER_LONG
		-----------------------------------------------------------------
		|		|		|		|		|
		|PGD		|PMD		|PTE		|OFFSET		|
		-----------------------------------------------------------------
		<---------------------PAGE_MASK-----------------><---PAGE_SIZE-->
		<------------PMD_MASK----------><------------PMD_SIZE----------->
		<--PGDIR_MASK--><-----------------PGDIR_SIZE-------------------->

#define PAGE_SHIFT      12
#define PAGE_SIZE       (1UL << PAGE_SHIFT)
#define PAGE_MASK       (~(PAGE_SIZE-1))


Describing a Page Table Entry
Each entry is described by the structs pte_t, pmd_t and pgd_t for PTEs, PMDs and
PGDs respectively. Even though these are often just unsigned integers, they are 
defined as structs for two reasons. 

The first is for type protection so that they will not be used inappropriately. 
The second is for features like PAE on the x86 where an additional 4 bits is used
for addressing more than 4GiB of memory. 

To store the protection bits, pgprot_t is defined which holds the relevant flags
and is usually stored in the lower bits of a page table entry.

For type casting, 4 macros are provided in asm/page.h, which takes the above types
and returns the relevant part of the structs. They are pte_val(), pmd_val(), pgd_val()
and pgprot_val().

 To reverse the type casting, 4 more macros are provided __pte(), __pmd(), __pgd() and
__pgprot().

On an x86 with no PAE, the pte_t is simply a 32 bit integer within a struct. Each pte_t
points to an address of a page frame and all the addresses pointed to are guaranteed to
be page aligned. Therefore, there are PAGE_SHIFT (12) bits in that 32 bit value that are
free for status bits of the page table entry.


Using Page Table Entries
To navigate the page directories, three macros are provided which break up a linear 
address space into its component parts. 

pgd_offset() takes an address and the mm_struct for the process and returns the PGD
entry that covers the requested address. 
pmd_offset() takes a PGD entry and an address and returns the relevant PMD. 
pte_offset() takes a PMD and returns the relevant PTE. 

The remainder of the linear address provided is the offset within the page. 

pte_none(), pmd_none() and pgd_none() return 1 if the corresponding entry does not exist
pte_present(), pmd_present() and pgd_present() return 1 if the corresponding page 
table entries have the PRESENT bit set
pte_clear(), pmd_clear() and pgd_clear() will clear the corresponding page table entry

pmd_bad() and pgd_bad() are used to check entries when passed as input parameters to
functions that may change the value of the entries. Whether it returns 1 varies between
the few architectures that define these macros but for those that actually define it,
making sure the page entry is marked as present and accessed are the two most important checks.

follow_page()

407         pgd_t *pgd;
408         pmd_t *pmd;
409         pte_t *ptep, pte;
410 
411         pgd = pgd_offset(mm, address);
412         if (pgd_none(*pgd) || pgd_bad(*pgd))
413                 goto out;
414 
415         pmd = pmd_offset(pgd, address);
416         if (pmd_none(*pmd) || pmd_bad(*pmd))
417                 goto out;
418 
419         ptep = pte_offset(pmd, address);
420         if (!ptep)
421                 goto out;
422 
423         pte = *ptep;

The read permissions for an entry are tested with pte_read(), set with pte_mkread()
and cleared with pte_rdprotect();
The write permissions are tested with pte_write(), set with pte_mkwrite() and 
cleared with pte_wrprotect();
The execute permissions are tested with pte_exec(), set with pte_mkexec() and 
cleared with pte_exprotect().
The permissions can be modified to a new value with pte_modify() but its use is 
almost non-existent. It is only used in the function change_pte_range()

The fourth set of macros examine and set the state of an entry. There are only
two bits that are important in Linux, the dirty bit and the accessed bit. 

To check these bits, the macros pte_dirty() and pte_young() macros are used. 
To set the bits, the macros pte_mkdirty() and pte_mkyoung() are used. 
To clear them, the macros pte_mkclean() and pte_old() are available.


Translating and Setting Page Table Entries
mk_pte() takes a struct page and protection bits and combines them 
together to form the pte_t

pte_page() returns the struct page which corresponds to the PTE entry.

pmd_page() returns the struct page containing the set of PTEs.

set_pte() takes a pte_t such as that returned by mk_pte() and 
places it within the processes page tables. 

pte_clear() is the reverse operation. 

ptep_get_and_clear() which clears an entry from the process page
table and returns the pte_t


Allocating and Freeing Page Tables
The pages used for the page tables are cached in a number of different
lists called quicklists.

PGDs, PMDs and PTEs have two sets of functions each for the allocation and
freeing of page tables. The allocation functions are pgd_alloc(), pmd_alloc()
and pte_alloc() respectively and the free functions are, predictably enough,
called pgd_free(), pmd_free() and pte_free().

If a page is not available from the cache, a page will be allocated using the
physical page allocator. The functions for the three levels of page tables
are get_pgd_slow(), pmd_alloc_one() and pte_alloc_one().


Kernel Page Tables
When the system first starts, paging is not enabled as page tables do not
magically initialise themselves. Each architecture implements this differently 
so only the x86 case will be discussed. 

The page table initialisation is divided into two phases. The bootstrap phase
sets up page tables for just 8MiB so the paging unit can be enabled. 
The second phase initialises the rest of the page tables.

Bootstrapping
The assembler function startup_32() is responsible for enabling the paging 
unit in arch/i386/kernel/head.S. 
While all normal kernel code in vmlinuz is compiled with the base address 
at PAGE_OFFSET + 1MiB, the kernel is actually loaded beginning at the first
megabyte (0x00100000) of memory. 

The first megabyte is used by some devices for communication with the BIOS and is skipped.

The bootstrap code in this file treats 1MiB as its base address by subtracting
__PAGE_OFFSET from any address until the paging unit is enabled so before the 
paging unit is enabled, a page table mapping has to be established which 
translates the 8MiB of physical memory to the virtual address PAGE_OFFSET.

Initialisation begins with statically defining at compile time an array called
swapper_pg_dir which is placed using linker directives at 0x00101000. It then
establishes page table entries for 2 pages, pg0 and pg1. 
The first pointers to pg0 and pg1 are placed to cover the region 1-9MiB the 
second pointers to pg0 and pg1 are placed at PAGE_OFFSET+1MiB. 
This means that when paging is enabled, they will map to the correct pages 
using either physical or virtual addressing for just the kernel image. 

The rest of the kernel page tables will be initialised by paging_init().


Finalising
			paging_init()
	pagetable_init	kmap_init	zones_size_init
	fixrange_init	kmap_get_fixmap_pte	
	alloc_bootmem_low_pages

The function first calls pagetable_init() to initialise the page tables 
necessary to reference all physical memory in ZONE_DMA and ZONE_NORMAL. 
Remember that high memory in ZONE_HIGHMEM cannot be directly referenced 
and mappings are set up for it temporarily. 

For each pgd_t used by the kernel, the boot memory allocator is called
to allocate a page for the PMDs

A page for PTEs will be allocated for each pmd_t.

If the CPU supports the PGE flag, it also will be set so that the page 
table entry will be global and visible to all processes.

pagetable_init() calls fixrange_init() to setup the fixed address space 
mappings at the end of the virtual address space starting at FIXADDR_START. 
These mappings are used for purposes such as the local APIC and the atomic
kmappings between FIX_KMAP_BEGIN and FIX_KMAP_END required by kmap_atomic().

calls fixrange_init() to initialise the page table entries required for 
normal high memory mappings with kmap().

Once pagetable_init() returns, the page tables for kernel space are now full
initialised so the static PGD (swapper_pg_dir) is loaded into the CR3 register
so that the static table is now being used by the paging unit.


Mapping addresses to a struct page
There is a requirement for Linux to have a fast method of mapping virtual addresses 
to physical addresses and for mapping struct pages to their physical address.

Linux achieves this by knowing where, in both virtual and physical memory, the
global mem_map array is as the global array has pointers to all struct pages
representing physical memory in the system. All architectures achieve this 
with very similar mechanisms.

Mapping Physical to Virtual Kernel Addresses

Linux sets up a direct mapping from the physical address 0 to the virtual 
address PAGE_OFFSET at 3GiB on the x86. This means that any virtual address
can be translated to the physical address by simply subtracting PAGE_OFFSET
which is essentially what the function virt_to_phys() with the macro __pa() i
does:

/* from <asm-i386/page.h> */
132 #define __pa(x)                 ((unsigned long)(x)-PAGE_OFFSET)

/* from <asm-i386/io.h> */
 76 static inline unsigned long virt_to_phys(volatile void * address)
 77 {
 78         return __pa(address);
 79 }

Obviously the reverse operation involves simply adding PAGE_OFFSET which 
is carried out by the function phys_to_virt() with the macro __va().

Mapping struct pages to Physical Addresses

The kernel image is located at the physical address 1MiB, which of course 
translates to the virtual address PAGE_OFFSET + 0x00100000 and a virtual 
region totaling about 8MiB is reserved for the image which is the region 
that can be addressed by two PGDs. This would imply that the first available 
memory to use is located at 0xC0800000 but that is not the case. 

Linux tries to reserve the first 16MiB of memory for ZONE_DMA so first virtual 
area used for kernel allocations is actually 0xC1000000. This is where the 
global mem_map is usually located. ZONE_DMA will be still get used, but only 
when absolutely necessary.

Physical addresses are translated to struct pages by treating them as an 
index into the mem_map array. 

Shifting a physical address PAGE_SHIFT bits to the right will treat it as 
a PFN from physical address 0 which is also an index within the mem_map 
array. This is exactly what the macro virt_to_page() does which is declared 
as follows in <asm-i386/page.h>:

#define virt_to_page(kaddr) (mem_map + (__pa(kaddr) >> PAGE_SHIFT))

The macro virt_to_page() takes the virtual address kaddr, converts it to 
the physical address with __pa(), converts it into an array index by bit 
shifting it right PAGE_SHIFT bits and indexing into the mem_map by simply 
adding them together.



Translation Lookaside Buffer (TLB)







