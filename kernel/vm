Descibing physical memory

NUMA: non-uniform memory access

With large scale machines, memory may be arranged into banks
that incur a different cost to access depending on the “distance”
from the processor. For example, there might be a bank of memory
assigned to each CPU or a bank of memory very suitable for DMA
near device cards.

Each bank is called a node and the concept is represented under
Linux by a struct pglist_data even if the architecture is UMA.
This struct is always referenced to by it's typedef pg_data_t.

Every node in the system is kept on a NULL terminated list called
pgdat_list and each node is linked to the next with the field 
pg_data_t->node_next. For UMA architectures like PC desktops,
only one static pg_data_t structure called contig_page_data is used. 

Each node is divided up into a number of blocks called zones which
represent ranges within memory. 

Zones should not be confused with zone based allocators as they are
unrelated. 

A zone is described by a struct zone_struct, typedeffed to zone_t
and each one is of type ZONE_DMA, ZONE_NORMAL or ZONE_HIGHMEM.

Each zone type suitable a different type of usage. 
ZONE_DMA is memory in the lower physical memory ranges which certain 
ISA devices require.
Memory within ZONE_NORMAL is directly mapped by the kernel into the 
upper region of the linear address space.
ZONE_HIGHMEM is the remaining available memory in the system and is
not directly mapped by the kernel.

With the x86 the zones are:

ZONE_DMA	First 16MiB of memory
ZONE_NORMAL	16MiB - 896MiB
ZONE_HIGHMEM	896 MiB - End

Each physical page frame is represented by a struct page and all the
structs are kept in a global mem_map array which is usually stored at
the beginning of ZONE_NORMAL or just after the area reserved for the
loaded kernel image in low memory machines.

			pg_data_t
			    |
			node_zones
			    |
	ZONE_DMA	ZONE_NORMAL	ZONE_HIGHMEM
	    |		    |		     |
	node_mem_map	node_mem_map	   node_mem_map
	    |		    |		     |
	   page		   page		    page


Nodes:
Each node in memory is described by a pg_data_t which is a typedef for
a struct pglist_data. When allocating a page, Linux uses a node-local 
allocation policy to allocate memory from the node closest to the running
CPU. As processes tend to run on the same CPU, it is likely the memory 
from the current node will be used. 

The struct is declared as follows in <linux/mmzone.h>

129 typedef struct pglist_data {
130     zone_t node_zones[MAX_NR_ZONES];		//The zones for this node, ZONE_HIGHMEM, ZONE_NORMAL, ZONE_DMA
131     zonelist_t node_zonelists[GFP_ZONEMASK+1];	//This is the order of zones that allocations are preferred from. 
							//A failed allocation in ZONE_HIGHMEM may fall back to ZONE_NORMAL or back to ZONE_DMA;
132     int nr_zones;					//Number of zones in this node, between 1 and 3
133     struct page *node_mem_map;			//
134     unsigned long *valid_addr_bitmap;		//A bitmap which describes “holes” in the memory node that no memory exists for. (Sparc and Sparc64)
135     struct bootmem_data *bdata;			//This is only of interest to the boot memory allocator
136     unsigned long node_start_paddr;/node_start_pfn	//The starting physical address of the node
137     unsigned long node_start_mapnr;			//gives the page offset within the global mem_map ?
138     unsigned long node_size;/node_present_pages and node_spanned_pages	//The total number of pages in this zone
139     int node_id;					//The Node ID (NID) of the node, starts at 0
140     struct pglist_data *node_next;			//Pointer to next node in a NULL terminated list
141 } pg_data_t; 

All nodes in the system are maintained on a list called pgdat_list. 

Initialized by the init_bootmem_core() function.
Macro for_each_online_pgdat() is used to traverse the list


Zones
Zones are described by a struct zone_struct and is usually referred
to by it's typedef zone_t. It keeps track of information like page 
usage statistics, free area information and locks.

It is declared as follows in <linux/mmzone.h>:

37 typedef struct zone_struct {
41     spinlock_t        lock;				//Spinlock to protect the zone from concurrent accesses
42     unsigned long     free_pages;			//Total number of free pages in the zone
43     unsigned long     pages_min, pages_low, pages_high; //zone watermarks
44     int               need_balance;			//tells the pageout kswapd to balance the zone
45 
49     free_area_t       free_area[MAX_ORDER];		//Free area bitmaps used by the buddy allocator
50 
76     wait_queue_head_t * wait_table;			//A hash table of wait queues of processes waiting on a page to be freed
77     unsigned long     wait_table_size;		//Number of queues in the hash table which is a power of 2
78     unsigned long     wait_table_shift;		//the number of bits in a long minus the binary logarithm of the table size above ??
79 
83     struct pglist_data *zone_pgdat;			//Points to the parent pg_data_t
84     struct page        *zone_mem_map;		//The first page in the global mem_map this zone refers to
85     unsigned long      zone_start_paddr;
86     unsigned long      zone_start_mapnr;
87 
91     char               *name;			//The string name of the zone, “DMA”, “Normal” or “HighMem”
92     unsigned long      size;				//The size of the zone in pages
93 } zone_t;

Zone Watermarks
When available memory in the system is low, the pageout daemon 
kswapd is woken up to start freeing pages

Each zone has three watermarks called pages_low, pages_min and 
pages_high which help track how much pressure a zone is under.

PAGES_LOW When pages_low number of free pages is reached, kswapd 
is woken up by the buddy allocator to start freeing pages. 
The value is twice the value of pages_min by default;

PAGES_MIN When pages_min is reached, the allocator will do the 
kswapd work in a synchronous fashion, sometimes referred to as 
the direct-reclaim path. 

PAGES_HIGH Once kswapd has been woken to start freeing pages it 
will not consider the zone to be “balanced” when pages_high pages 
are free. Once the watermark has been reached, kswapd will go back 
to sleep. 
The default for pages_high is three times the value of pages_min.


Calculating the Size of Zones
			setup_memory

find_max_pfn find_max_low_pfn	init_bootmem  register_bootmem_low_pages	find_smp_config

				init_bootmem_core	free_bootmem	  	find_intel_smp
							free_bootmem_cores smp_	scan_config
  									reserve_bootmem 
									reserve_bootmem_core

The pfn is an offset, counted in pages, within the physical memory map.
The first PFN usable by the system, min_low_pfn is located at the beginning
of the first page after _end which is the end of the loaded kernel image.
The value is stored as a file scope variable in mm/bootmem.c for use with
the boot memory allocator.

Zone Wait Queue Table
When IO is beginning performed on a page, such are during page-in or page-out,
it's locked to prevent accessing it with inconsistent data. Processor wishing
to use it have to join a wait queue before it can be accessed by calling
wait_on_page(). When the IO is completed, the page will be unlocked with
UnlockPage() and any process waiting on the queue will be woken up. Each page
could have a wait queue but it would be very expensive in terms of memory to 
have so many separate queues so instead, the wait queue is stored in zone_t.


Process A wants to lock page

__wait_on_page 		--->  page_waitqueue --->	struct page
							page->flags
			add_wait_queue()
Sleeping 						page_zone()
							wait_queue_head_t
							wait_queue_head_t
							wait_queue_head_t
							zone->wait_table

Zone Initialization
The zones are initialised after the kernel page tables have been fully setup by paging_init(). 

nid is the Node ID which is the logical identifier of the node whose zones are being initialised;
pgdat is the node's pg_data_t that is being initialised. In UMA, this will simply be contig_page_data;
pmap is set later by free_area_init_core() to point to the beginning of the local lmem_map array allocated for the node.
zones_sizes is an array containing the size of each zone in pages
zone_start_paddr is the starting physical address for the first zone;
zone_holes is an array containing the total size of memory holes in the zones


Inintializing mem_map
The mem_map area is created during system startup in one of two fashions.
On NUMA systems, the global mem_map is treated as a virtual array starting at PAGE_OFFSET.
free_area_init_node() is called for each active node in the system which allocates 
the portion of this array for the node being initialised.

On UMA systems, free_area_init() is uses contig_page_data as the node and the 
global mem_map as the “local” mem_map for this node. 


PAGES
Every physical page frame in the system has an associated struct page which is 
used to keep track of its status.

152 typedef struct page {
153     struct list_head list;		//Pages may belong to many lists and this field is used as the list head.
154     struct address_space *mapping;	//When files or devices are memory mapped, their inode has an associated address_space.
155     unsigned long index;		//This field has two uses and it depends on the state of the page what it means.
					//If the page is part of a file mapping, it is the offset within the file. If the page is 
					//part of the swap cache this will be the offset within the address_space for the swap address space
156     struct page *next_hash;		//Pages that are part of a file mapping are hashed on the inode and offset. 
158     atomic_t count;			//The reference count to the page
159     unsigned long flags;		// flags which describe the status of the page.
161     struct list_head lru;		//For the page replacement policy, pages that may be swapped out will exist on 
					//either the active_list or the inactive_list
163     struct page **pprev_hash;	//
164     struct buffer_head * buffers;	//If a page has buffers for a block device associated with it, this field is 
					//used to keep track of the buffer_head
175
176 #if defined(CONFIG_HIGHMEM) || defined(WANT_PAGE_VIRTUAL)
177     void *virtual;
179 #endif /* CONFIG_HIGMEM || WANT_PAGE_VIRTUAL */
180 } mem_map_t;


Mapping Pages to Zones
set_page_zone(page, nid * MAX_NR_ZONES + j);


High Memory
As the addresses space usable by the kernel (ZONE_NORMAL) is limited in size, the 
kernel has support for the concept of High Memory. 

To access memory between the range of 1GiB and 4GiB, the kernel temporarily maps 
pages from high memory into ZONE_NORMAL with kmap().



Page Table Management
Describing the Page Directory
Each process a pointer (mm_struct->pgd) to its own Page Global Directory (PGD) which
is a physical page frame. This frame contains an array of type pgd_t which is an 
architecture specific type defined in <asm/page.h>.

On the x86, the process page table is loaded by copying mm_struct->pgd into the cr3 
register which has the side effect of flushing the TLB. In fact this is how the function
__flush_tlb() is implemented in the architecture dependent code.

Each active entry in the PGD table points to a page frame containing an array of Page
Middle Directory (PMD) entries of type pmd_t which in turn points to page frames containing
Page Table Entries (PTE) of type pte_t, which finally points to page frames containing the
actual user data. 

In the event the page has been swapped out to backing storage, the swap entry is
stored in the PTE and used by do_swap_page() during page fault to find the swap 
entry containing the page data. 

				linear address
offset within process PGD	offset within PMD page frame	offset within PTE page frame	offset within Date frame
	|				|				|				|	________
	|				|				|				|	|	|
	|				|				|				------->|	|
	|				|				|		________		|	|
	|				|				|		|	|		|	|
	|				|				pte_offset()--->|pte_t	|-------------->|_______|
	|				|						|	|		Page frame
	|				|		 ________			|	|		with user date
	|				|		 |	|			|	|
	|				pmd_offset()---->|pmd_t |---------------------->|_______|
	|						 |	|			pte_t page
	|     	       _______				 |	|			frame
	|	       |      |				 |	|
pgd_offset()---------->|pgd_t |------------------------->|______|
  		|      |      |				pmd_t page
		|      |      |				frame
	    pdg_index()|      |
		|      |      |
mm_struct->pgd--------->______|
			only 1 
			pgd_t page frame


Any given linear address may be broken up into parts to yield offsets
within these three page table levels and an offset within the actual page.

The SHIFT macros specifies the length in bits that are mapped by each level of the page tables.

					linear address
					BITS_PER_LONG
		________________________________________________________________
		|		|		|		|		|
		|PGD		|PMD		|PTE		|OFFSET		|
		-----------------------------------------------------------------
								<--PAGE_SHIFT--->
						<-----------PMD_SHIFT----------->
				<--------------------PGDIR_SHHIFT--------------->


The MASK values can be AND with a linear address to mask out all the
upper bits and is frequently used to determine if a linear address
is aligned to a given level within the page table.

The SIZE macros reveal how many bytes are addressed by each entry at each level.

					Linear address
					BITS_PER_LONG
		-----------------------------------------------------------------
		|		|		|		|		|
		|PGD		|PMD		|PTE		|OFFSET		|
		-----------------------------------------------------------------
		<---------------------PAGE_MASK-----------------><---PAGE_SIZE-->
		<------------PMD_MASK----------><------------PMD_SIZE----------->
		<--PGDIR_MASK--><-----------------PGDIR_SIZE-------------------->

#define PAGE_SHIFT      12
#define PAGE_SIZE       (1UL << PAGE_SHIFT)
#define PAGE_MASK       (~(PAGE_SIZE-1))


Describing a Page Table Entry
Each entry is described by the structs pte_t, pmd_t and pgd_t for PTEs, PMDs and
PGDs respectively. Even though these are often just unsigned integers, they are 
defined as structs for two reasons. 

The first is for type protection so that they will not be used inappropriately. 
The second is for features like PAE on the x86 where an additional 4 bits is used
for addressing more than 4GiB of memory. 

To store the protection bits, pgprot_t is defined which holds the relevant flags
and is usually stored in the lower bits of a page table entry.

For type casting, 4 macros are provided in asm/page.h, which takes the above types
and returns the relevant part of the structs. They are pte_val(), pmd_val(), pgd_val()
and pgprot_val().

 To reverse the type casting, 4 more macros are provided __pte(), __pmd(), __pgd() and
__pgprot().

On an x86 with no PAE, the pte_t is simply a 32 bit integer within a struct. Each pte_t
points to an address of a page frame and all the addresses pointed to are guaranteed to
be page aligned. Therefore, there are PAGE_SHIFT (12) bits in that 32 bit value that are
free for status bits of the page table entry.


Using Page Table Entries
To navigate the page directories, three macros are provided which break up a linear 
address space into its component parts. 

pgd_offset() takes an address and the mm_struct for the process and returns the PGD
entry that covers the requested address. 
pmd_offset() takes a PGD entry and an address and returns the relevant PMD. 
pte_offset() takes a PMD and returns the relevant PTE. 

The remainder of the linear address provided is the offset within the page. 

pte_none(), pmd_none() and pgd_none() return 1 if the corresponding entry does not exist
pte_present(), pmd_present() and pgd_present() return 1 if the corresponding page 
table entries have the PRESENT bit set
pte_clear(), pmd_clear() and pgd_clear() will clear the corresponding page table entry

pmd_bad() and pgd_bad() are used to check entries when passed as input parameters to
functions that may change the value of the entries. Whether it returns 1 varies between
the few architectures that define these macros but for those that actually define it,
making sure the page entry is marked as present and accessed are the two most important checks.

follow_page()

407         pgd_t *pgd;
408         pmd_t *pmd;
409         pte_t *ptep, pte;
410 
411         pgd = pgd_offset(mm, address);
412         if (pgd_none(*pgd) || pgd_bad(*pgd))
413                 goto out;
414 
415         pmd = pmd_offset(pgd, address);
416         if (pmd_none(*pmd) || pmd_bad(*pmd))
417                 goto out;
418 
419         ptep = pte_offset(pmd, address);
420         if (!ptep)
421                 goto out;
422 
423         pte = *ptep;

The read permissions for an entry are tested with pte_read(), set with pte_mkread()
and cleared with pte_rdprotect();
The write permissions are tested with pte_write(), set with pte_mkwrite() and 
cleared with pte_wrprotect();
The execute permissions are tested with pte_exec(), set with pte_mkexec() and 
cleared with pte_exprotect().
The permissions can be modified to a new value with pte_modify() but its use is 
almost non-existent. It is only used in the function change_pte_range()

The fourth set of macros examine and set the state of an entry. There are only
two bits that are important in Linux, the dirty bit and the accessed bit. 

To check these bits, the macros pte_dirty() and pte_young() macros are used. 
To set the bits, the macros pte_mkdirty() and pte_mkyoung() are used. 
To clear them, the macros pte_mkclean() and pte_old() are available.


Translating and Setting Page Table Entries
mk_pte() takes a struct page and protection bits and combines them 
together to form the pte_t

pte_page() returns the struct page which corresponds to the PTE entry.

pmd_page() returns the struct page containing the set of PTEs.

set_pte() takes a pte_t such as that returned by mk_pte() and 
places it within the processes page tables. 

pte_clear() is the reverse operation. 

ptep_get_and_clear() which clears an entry from the process page
table and returns the pte_t


Allocating and Freeing Page Tables
The pages used for the page tables are cached in a number of different
lists called quicklists.

PGDs, PMDs and PTEs have two sets of functions each for the allocation and
freeing of page tables. The allocation functions are pgd_alloc(), pmd_alloc()
and pte_alloc() respectively and the free functions are, predictably enough,
called pgd_free(), pmd_free() and pte_free().

If a page is not available from the cache, a page will be allocated using the
physical page allocator. The functions for the three levels of page tables
are get_pgd_slow(), pmd_alloc_one() and pte_alloc_one().


Kernel Page Tables
When the system first starts, paging is not enabled as page tables do not
magically initialise themselves. Each architecture implements this differently 
so only the x86 case will be discussed. 

The page table initialisation is divided into two phases. The bootstrap phase
sets up page tables for just 8MiB so the paging unit can be enabled. 
The second phase initialises the rest of the page tables.

Bootstrapping
The assembler function startup_32() is responsible for enabling the paging 
unit in arch/i386/kernel/head.S. 
While all normal kernel code in vmlinuz is compiled with the base address 
at PAGE_OFFSET + 1MiB, the kernel is actually loaded beginning at the first
megabyte (0x00100000) of memory. 

The first megabyte is used by some devices for communication with the BIOS and is skipped.

The bootstrap code in this file treats 1MiB as its base address by subtracting
__PAGE_OFFSET from any address until the paging unit is enabled so before the 
paging unit is enabled, a page table mapping has to be established which 
translates the 8MiB of physical memory to the virtual address PAGE_OFFSET.

Initialisation begins with statically defining at compile time an array called
swapper_pg_dir which is placed using linker directives at 0x00101000. It then
establishes page table entries for 2 pages, pg0 and pg1. 
The first pointers to pg0 and pg1 are placed to cover the region 1-9MiB the 
second pointers to pg0 and pg1 are placed at PAGE_OFFSET+1MiB. 
This means that when paging is enabled, they will map to the correct pages 
using either physical or virtual addressing for just the kernel image. 
 
The rest of the kernel page tables will be initialised by paging_init().


Finalising
			paging_init()
	pagetable_init	kmap_init	zones_size_init
	fixrange_init	kmap_get_fixmap_pte	
	alloc_bootmem_low_pages

The function first calls pagetable_init() to initialise the page tables 
necessary to reference all physical memory in ZONE_DMA and ZONE_NORMAL. 
Remember that high memory in ZONE_HIGHMEM cannot be directly referenced 
and mappings are set up for it temporarily. 

For each pgd_t used by the kernel, the boot memory allocator is called
to allocate a page for the PMDs

A page for PTEs will be allocated for each pmd_t.

If the CPU supports the PGE flag, it also will be set so that the page 
table entry will be global and visible to all processes.

pagetable_init() calls fixrange_init() to setup the fixed address space 
mappings at the end of the virtual address space starting at FIXADDR_START. 
These mappings are used for purposes such as the local APIC and the atomic
kmappings between FIX_KMAP_BEGIN and FIX_KMAP_END required by kmap_atomic().

calls fixrange_init() to initialise the page table entries required for 
normal high memory mappings with kmap().

Once pagetable_init() returns, the page tables for kernel space are now full
initialised so the static PGD (swapper_pg_dir) is loaded into the CR3 register
so that the static table is now being used by the paging unit.


Mapping addresses to a struct page
There is a requirement for Linux to have a fast method of mapping virtual addresses 
to physical addresses and for mapping struct pages to their physical address.

Linux achieves this by knowing where, in both virtual and physical memory, the
global mem_map array is as the global array has pointers to all struct pages
representing physical memory in the system. All architectures achieve this 
with very similar mechanisms.

Mapping Physical to Virtual Kernel Addresses

Linux sets up a direct mapping from the physical address 0 to the virtual 
address PAGE_OFFSET at 3GiB on the x86. This means that any virtual address
can be translated to the physical address by simply subtracting PAGE_OFFSET
which is essentially what the function virt_to_phys() with the macro __pa()
does:

/* from <asm-i386/page.h> */
132 #define __pa(x)                 ((unsigned long)(x)-PAGE_OFFSET)

/* from <asm-i386/io.h> */
 76 static inline unsigned long virt_to_phys(volatile void * address)
 77 {
 78         return __pa(address);
 79 }

Obviously the reverse operation involves simply adding PAGE_OFFSET which 
is carried out by the function phys_to_virt() with the macro __va().

Mapping struct pages to Physical Addresses

The kernel image is located at the physical address 1MiB, which of course 
translates to the virtual address PAGE_OFFSET + 0x00100000 and a virtual 
region totaling about 8MiB is reserved for the image which is the region 
that can be addressed by two PGDs. This would imply that the first available 
memory to use is located at 0xC0800000 but that is not the case. 

Linux tries to reserve the first 16MiB of memory for ZONE_DMA so first virtual 
area used for kernel allocations is actually 0xC1000000. This is where the 
global mem_map is usually located. ZONE_DMA will be still get used, but only 
when absolutely necessary.

Physical addresses are translated to struct pages by treating them as an 
index into the mem_map array. 

Shifting a physical address PAGE_SHIFT bits to the right will treat it as 
a PFN from physical address 0 which is also an index within the mem_map 
array. This is exactly what the macro virt_to_page() does which is declared 
as follows in <asm-i386/page.h>:

#define virt_to_page(kaddr) (mem_map + (__pa(kaddr) >> PAGE_SHIFT))

The macro virt_to_page() takes the virtual address kaddr, converts it to 
the physical address with __pa(), converts it into an array index by bit 
shifting it right PAGE_SHIFT bits and indexing into the mem_map by simply 
adding them together.



Translation Lookaside Buffer (TLB)
Initially, when the processor needs to map a virtual address to a physical 
address, it must traverse the full page directory searching for the PTE of 
interest. This would normally imply that each assembly instruction that 
references memory actually requires several separate memory references 
for the page table traversal.

To avoid this considerable overhead, architectures take advantage of the 
fact that most processes exhibit a locality of reference or, in other words, 
large numbers of memory references tend to be for a small number of pages.

They take advantage of this reference locality by providing a Translation 
Lookaside Buffer (TLB) which is a small associative memory that caches 
virtual to physical page table resolutions.

Linux assumes that the most architectures support some type of TLB although 
the architecture independent code does not cares how it works. Instead, 
architecture dependant hooks are dispersed throughout the VM code at points 
where it is known that some hardware with a TLB would need to perform a TLB 
related operation. 

For example, when the page tables have been updated, such as after a page 
fault has completed, the processor may need to be update the TLB for that 
virtual address mapping.

Not all architectures require these type of operations but because some do, 
the hooks have to exist. If the architecture does not require the operation 
to be performed, the function for that TLB operation will a null operation 
that is optimised out at compile time

void flush_tlb_all(void)
This flushes the entire TLB on all processors running in the system making 
it the most expensive TLB flush operation. After it completes, all modifications 
to the page tables will be visible globally. This is required after the 
kernel page tables, which are global in nature, have been modified such 
as after vfree() completes or after the PKMap is flushed.

void flush_tlb_mm(struct mm_struct *mm)
This flushes all TLB entries related to the userspace portion for the 
requested mm context.

In some architectures, such as MIPS, this will need to be performed for 
all processors but usually it is confined to the local processor. This is 
only called when an operation has been performed that affects the entire 
address space, such as after all the address mapping have been duplicated 
with dup_mmap() for fork or after all memory mappings have been deleted 
with exit_mmap().

void flush_tlb_range(struct mm_struct *mm, unsigned long start, unsigned long end)
this flushes all entries within the requested userspace range for the mm 
context. This is used after a new region has been moved or changeh as during 
mremap() which moves regions or mprotect() which changes the permissions. 
The function is also indirectly used during unmapping a region with munmap() 
which calls tlb_finish_mmu() which tries to use flush_tlb_range() intelligently. 
This API is provided for architectures that can remove ranges of TLB entries 
quickly rather than iterating with flush_tlb_page().

void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr)
This API is responsible for flushing a single page from the TLB. The two most 
common usage of it is for flushing the TLB after a page has been faulted 
in or has been paged out.


void flush_tlb_pgtables(struct mm_struct *mm, unsigned long start, unsigned long end)
This API is called with the page tables are being torn down and freed. Some 
platforms cache the lowest level of the page table, i.e. the actual page frame 
storing entries, which needs to be flushed when the pages are being deleted. 
This is called when a region is being unmapped and the page directory entries 
are being reclaimed.

void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr, pte_t pte)
This API is only called after a page fault completes. It tells the architecture 
dependant code that a new translation now exists at pte for the virtual address addr. 
It is up to each architecture how this information should be used. For example, 
Sparc64 uses the information to decide if the local CPU needs to flush it's data 
cache or does it need to send an IPI to a remote processor.



Level 1 CPU Cache Management
CPU caches, like TLB caches, take advantage of the fact that programs tend to 
exhibit a locality of reference.

To avoid having to fetch data from main memory for each reference, the CPU 
will instead cache very small amounts of data in the CPU cache. 

Frequently, there is two levels called the Level 1 and Level 2 CPU caches. 
The Level 2 CPU caches are larger but slower than the L1 cache but Linux 
only concerns itself with the Level 1 or L1 cache.

CPU caches are organised into lines. Each line is typically quite small, 
usually 32 bytes and each line is aligned to it's boundary size. In other words, 
a cache line of 32 bytes will be aligned on a 32 byte address.

With Linux, the size of the line is L1_CACHE_BYTES which is defined by each architecture.


How addresses are mapped to cache lines vary between architectures but the 
mappings come under three headings, direct mapping, associative mapping and 
set associative mapping. 

Direct mapping is the simpliest approach where each block of memory maps to only 
one possible cache line. 

With associative mapping, any block of memory can map to any cache line. 

Set associative mapping is a hybrid approach where any block of memory can 
may to any line but only within a subset of the available lines. 

Regardless of the mapping scheme, they each have one thing in common, addresses 
that are close together and aligned to the cache size are likely to use different 
lines. Hence Linux employs simple tricks to try and maximise cache usage.

Frequently accessed structure fields are at the start of the structure to 
increase the chance that only one line is needed to address the common fields;

Unrelated items in a structure should try to be at least cache size bytes 
apart to avoid false sharing between CPUs;

Objects in the general caches, such as the mm_struct cache, are aligned to 
the L1 CPU cache to avoid false sharing.

If the CPU references an address that is not in the cache, a cache missccurs 
and the data is fetched from main memory. The cost of cache misses is quite 
high as a reference to cache can typically be performed in less than 10ns 
where a reference to main memory typically will cost between 100ns and 200ns. 

The basic objective is then to have as many cache hits and as few cache misses 
as possible.

Just as some architectures do not automatically manage their TLBs, some do 
not automatically manage their CPU caches. The hooks are placed in locations 
where the virtual to physical mapping changes, such as during a page table 
update. The CPU cache flushes should always take place first as some CPUs 
require a virtual to physical mapping to exist when the virtual address 
is being flushed from the cache. 

void flush_cache_all(void)
This flushes the entire CPU cache system making it the most severe 
flush operation to use. It is used when changes to the kernel page 
tables, which are global in nature, are to be performed.

void flush_cache_mm(struct mm_struct mm)
This flushes all entires related to the address space. On completion, 
no cache lines will be associated with mm.

void flush_cache_range(struct mm_struct *mm, unsigned long start, unsigned long end)
This flushes lines related to a range of addresses in the address space. 
Like it's TLB equivilant, it is provided in case the architecture has an 
efficent way of flushing ranges instead of flushing each individual page.

void flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr)
This is for flushing a single page sized region. The VMA is supplied as 
the mm_struct is easily accessible via vma->vm_mm. Additionaly, by testing
for the VM_EXEC flag, the architecture will know if the region is executable 
for caches that separate the instructions and data caches.


A second set of interfaces is required to avoid virtual aliasing problems. 
The problem is that some CPUs select lines based on the virtual address 
meaning that one physical address can exist on multiple lines leading to 
cache coherency problems. Architectures with this problem may try and ensure 
that shared mappings will only use addresses as a stop-gap measure.


void flush_dcache_page(struct page *page
This function is called when the kernel writes to or copies from a page cache 
page as these are likely to be mapped by multiple processes.

void flush_icache_range(unsigned long address, unsigned long endaddr)
This is called when the kernel stores information in addresses that is 
likely to be executed, such as when a kermel module has been loaded.

void flush_icache_page(struct vm_area_struct *vma, struct page *page)
This is called when a page-cache page is about to be mapped. It is up 
to the architecture to use the VMA flags to determine whether the 
I-Cache or D-Cache should be flushed.



Process Address Space
One of the principal advantages of virtual memory is that each process 
has its own virtual address space, which is mapped to physical memory 
by the operating system.

Zero page
The kernel treats the userspace portion of the address space very 
differently to the kernel portion. For example, allocations for the 
kernel are satisfied immediately and are visible globally no matter 
what process is on the CPU. 

vmalloc() is partially an exception as a minor page fault will occur 
to sync the process page tables with the reference page tables, but 
the page will still be allocated immediately upon request. 

With a process, space is simply reserved in the linear address space 
by pointing a page table entry to a read-only globally visible page 
filled with zeros. On writing, a page fault is triggered which results 
in a new page being allocated, filled with zeros, placed in the page 
table entry and marked writable. It is filled with zeros so that the 
new page will appear exactly the same as the global zero-filled page.


The userspace portion is not trusted or presumed to be constant. After 
each context switch, the userspace portion of the linear address space 
can potentially change except when a Lazy TLB switch is used.

Linear Address Space
From a user perspective, the address space is a flat linear address 
space but predictably, the kernel's perspective is very different. 

The address space is split into two parts, the userspace part which 
potentially changes with each full context switch and the kernel address 
space which remains constant. The location of the split is determined by 
the value of PAGE_OFFSET which is at 0xC0000000 on the x86. 

This means that 3GiB is available for the process to use while the 
remaining 1GiB is always mapped by the kernel.

<----------Physical Memory Map---->                   <--------------VMALLOC_REVERSE-------------|

<-TASK_SIZE->|			    <-VMALLOC_OFFSET->
Process	     | KERNEL |struct page |	Pages         |vmalloc|Pages|	kmap 	| FIXED VIRTUAL |
Address space| image  |Map(mem_map)|	Gap	      |address|Gap  |	address	|ADDRESS MAPPING|
0	     |					      |space  |     |	space	|   		|
	PAGE_OFFSET				VMALLOC_START V_END PKMAP_BASE FIXADDR_START FIXADDR_TOP

8MiB (the amount of memory addressed by two PGDs) is reserved at 
PAGE_OFFSET for loading the kernel image to run.

8MiB is simply a reasonable amount of space to reserve for the 
purposes of loading the kernel image. The kernel image is placed 
in this reserved space during kernel page tables initialisation.

The location of the array is usually at the 16MiB mark to avoid 
using ZONE_DMA but not always.

The region between PAGE_OFFSET and VMALLOC_START - VMALLOC_OFFSET is 
the physical memory map and the size of the region depends on the 
amount of available RAM. 

As the running kernel needs these functions, a region of at least 
VMALLOC_RESERVE will be reserved at the top of the address space. 
VMALLOC_RESERVE is architecture specific but on the x86, it is 
defined as 128MiB. This is why ZONE_NORMAL is generally referred 
to being only 896MiB in size; it is the 1GiB of the upper potion 
of the linear address space minus the minimum 128MiB that is reserved 
for the vmalloc region.


Managing the Address Space

----->struct mm_struct ---> struct mm_struct ---> struct mm_struct --->
			   mmap	/	\
			       /         \vm_mm
		struct vm_area_struct -> struct vm_area_struct
			^	|    vm_next	|
			|	|		|
			|	|__struct file _|
			|		|
			|		|f_dentry
			|	  struct dentry
			|	    ^       |d_inode
			|  i_dentry |       | 
			|	  struct inode
			|		|
			|		|i_mapping
			---i_mmap-----	struct address_space----a_ops---
						^			|
						|
						|		struct address_space_operations
				mapping	--------|----------
				struct page		struct page


fork():	Creates a new process with a new address space. All the 
pages are marked COW and are shared between the two processes 
until a page fault occurs to make private copies.

clone():clone() allows a new process to be created that shares 
parts of its context with its parent and is how threading is 
implemented in Linux. clone() without the CLONE_VM set will 
create a new address space which is essentially the same as fork()

mmap(): mmap() creates a new region within the process linear address space

mremap(): Remaps or resizes a region of memory. If the virtual address 
space is not available for the mapping, the region may be moved unless 
the move is forbidden by the caller.

munmap(): This destroys part or all of a region. If the region been 
unmapped is in the middle of an existing region, the existing region 
is split into two separate regions

shmat(): This attaches a shared memory segment to a process address space

shmdt(): Removes a shared memory segment from an address space

execve(): This loads a new executable file replacing the current address space

exit(): Destroys an address space and all regions



Process Address Space Descriptor
The process address space is described by the mm_struct struct meaning that 
only one exists for each process and is shared between userspace threads. 
In fact, threads are identified in the task list by finding all task_structs 
which have pointers to the same mm_struct.

A unique mm_struct is not needed for kernel threads as they will never page
fault or access the userspace portion. The only exception is page faulting
within the vmalloc space. The page fault handling code treats this as a special
case and updates the current page table with information in the the master page
table. As a mm_struct is not needed for kernel threads, the task_struct->mm
field for kernel threads is always NULL. For some tasks such as the boot idle
task, the mm_struct is never setup but for kernel threads, a call to
daemonize() will call exit_mm() to decrement the usage counter.

As TLB flushes are extremely expensive, especially with architectures such 
as the PPC, a technique called lazy TLB is employed which avoids unnecessary 
TLB flushes by processes which do not access the userspace page tables as the 
kernel portion of the address space is always visible. The call to switch_mm(), 
which results in a TLB flush, is avoided by “borrowing” the mm_struct used by 
the previous task and placing it in task_struct->active_mm. This technique has 
made large improvements to context switches times.

When entering lazy TLB, the function enter_lazy_tlb() is called to ensure 
that a mm_struct is not shared between processors in SMP machines, making it 
a NULL operation on UP machines. The second time use of lazy TLB is during process 
exit when start_lazy_tlb() is used briefly while the process is waiting to 
be reaped by the parent.

The struct has two reference counts called mm_users and mm_count for two types 
of “users”. mm_users is a reference count of processes accessing the userspace 
portion of for this mm_struct, such as the page tables and file mappings. Threads 
and the swap_out() code for instance will increment this count making sure a 
mm_struct is not destroyed early. When it drops to 0, exit_mmap() will delete 
all mappings and tear down the page tables before decrementing the mm_count.

mm_count is a reference count of the “anonymous users” for the mm_struct initialised 
at 1 for the “real” user. An anonymous user is one that does not necessarily care 
about the userspace portion and is just borrowing the mm_struct. Example users 
are kernel threads which use lazy TLB switching. When this count drops to 0, 
the mm_struct can be safely destroyed. Both reference counts exist because anonymous 
users need the mm_struct to exist even if the userspace mappings get destroyed and 
there is no point delaying the teardown of the page tables.

206 struct mm_struct {
207     struct vm_area_struct * mmap;		//The head of a linked list of all VMA regions in the address space
208     rb_root_t mm_rb;			//VMAs are arranged in a linked list and in a red-black tree for fast lookups. Root of the tree
209     struct vm_area_struct * mmap_cache;	//The VMA found during the last call to find_vma() is stored in this field on 
						//the assumption that the area will be used again soon
210     pgd_t * pgd;				//The Page Global Directory for this process
211     atomic_t mm_users;			//A reference count of users accessing the userspace portion of the address space
212     atomic_t mm_count;			//A reference count of the anonymous users for the mm_struct starting at 1 for the “real” user
213     int map_count;				//Number of VMAs in use
214     struct rw_semaphore mmap_sem;		//This is a long lived lock which protects the VMA list for readers and writers. 
215     spinlock_t page_table_lock;		//This protects most fields on the mm_struct. 
216 
217     struct list_head mmlist;		//All mm_structs are linked together via this field;
221 
222     unsigned long start_code, end_code, start_data, end_data;	//The start and end address of the code section, data section
223     unsigned long start_brk, brk, start_stack;			//The start and end address of the heap, the start of the stack region
224     unsigned long arg_start, arg_end, env_start, env_end;		//
225     unsigned long rss, total_vm, locked_vm;				//Resident Set Size is the number of resident pages for this process
226     unsigned long def_flags;
227     unsigned long cpu_vm_mask;				//A bitmask representing all possible CPUs in an SMP system.
228     unsigned long swap_address;				//Used by the pageout daemon to record the last address 
								//that was swapped from when swapping out entire processes;
229 
230     unsigned dumpable:1;
231 
232     /* Architecture-specific MM context */
233     mm_context_t context;					//Architecture specific MMU context.
234 };


mm_init(): Initialises a mm_struct by setting starting values for each field, allocating a PGD, initialising spinlocks etc
allocate_mm(): Allocates a mm_struct() from the slab allocator
mm_alloc(): Allocates a mm_struct using allocate_mm() and calls mm_init() to initialise it
exit_mmap(): Walks through a mm_struct and unmaps all VMAs associated with it
copy_mm(): Makes an exact copy of the current tasks mm_struct for a new task. This is only used during fork
free_mm(): Returns the mm_struct to the slab allocator


Initialising a Descriptor
The initial mm_struct in the system is called init_mm() and is statically 
initialised at compile time using the macro INIT_MM().

238 #define INIT_MM(name) \
239 {                                                       \
240     mm_rb:          RB_ROOT,                            \
241     pgd:            swapper_pg_dir,                     \
242     mm_users:       ATOMIC_INIT(2),                     \
243     mm_count:       ATOMIC_INIT(1),                     \
244     mmap_sem:       __RWSEM_INITIALIZER(name.mmap_sem), \
245     page_table_lock: SPIN_LOCK_UNLOCKED,                \
246     mmlist:         LIST_HEAD_INIT(name.mmlist),        \
247 }
Once it is established, new mm_structs are created using their parent 
mm_struct as a template. The function responsible for the copy operation 
is copy_mm() and it uses init_mm() to initialise process specific fields.

Destroying a Descriptor
While a new user increments the usage count with atomic_inc(&mm->mm_users), 
it is decremented with a call to mmput().

If the mm_users count reaches zero, all the mapped regions are destroyed with 
exit_mmap() and the page tables destroyed as there is no longer any users of 
the userspace portions.

The mm_count count is decremented with mmdrop() as all the users of the page 
tables and VMAs are counted as one mm_struct user. When mm_count reaches zero, 
the mm_struct will be destroyed.


Memory Regions
The full address space of a process is rarely used, only sparse regions are. 
Each region is represented by a vm_area_struct which never overlap and represent 
a set of addresses with the same protection and purpose. 

Examples of a region include a read-only shared library loaded into the address 
space or the process heap. A full list of mapped regions a process has may be 
viewed via the proc interface at /proc/PID/maps where PID is the process ID of 
the process that is to be examined.

If the region is backed by a file, the struct file is available through the vm_file 
field which has a pointer to the struct inode. The inode is used to get the struct 
address_space which has all the private information about the file including a set 
of pointers to filesystem functions which perform the filesystem specific operations 
such as reading and writing pages to disk.

 44 struct vm_area_struct {
 45     struct mm_struct * vm_mm;		//The mm_struct this VMA belongs to
 46     unsigned long vm_start;			//The starting address of the region
 47     unsigned long vm_end;			//The end address of the region
 49 					
 50     /* linked list of VM areas per task, sorted by address */
 51     struct vm_area_struct *vm_next;		//All the VMAs in an address space are linked together 
						//in an address-ordered singly linked list via this field
 52 
 53     pgprot_t vm_page_prot;			//The protection flags that are set for each PTE in this VMA
 54     unsigned long vm_flags;			//A set of flags describing the protections and properties of the VMA
 55 
 56     rb_node_t vm_rb;			//all the VMAs are stored on a red-black tree for fast lookups
 57 
 63     struct vm_area_struct *vm_next_share;	//Shared VMA regions based on file mappings linked together with this field
 64     struct vm_area_struct **vm_pprev_share;	//The complement of vm_next_share
 65 
 66     /* Function pointers to deal with this struct. */
 67     struct vm_operations_struct * vm_ops;	//The vm_ops field contains functions pointers for open(), 
						//close() and nopage(). These are needed for syncing with information from the disk;
 68 
 69     /* Information about our backing store: */
 70     unsigned long vm_pgoff;			//This is the page aligned offset within a file that is memory mapped
 72     struct file * vm_file;			//The struct file pointer to the file being mapped
 73     unsigned long vm_raend;			//This is the end address of a read-ahead window
 74     void * vm_private_data;			//Used by some device drivers to store private information.
 75 };

All the regions are linked together on a linked list ordered by address 
via the vm_next field. When searching for a free area, it is a simple 
matter of traversing the list but a frequent operation is to search 
for the VMA for a particular address such as during page faulting for 
example. In this case, the red-black tree is traversed as it has O(logN) 
search time on average. 

The tree is ordered so that lower addresses than the current node are 
on the left leaf and higher addresses are on the right.


Memory Region Operations
There are three operations which a VMA may support called open(), close() 
and nopage(). It supports these with a vm_operations_struct in the VMA 
called vma->vm_ops.

truct vm_operations_struct {
134     void (*open)(struct vm_area_struct * area);
135     void (*close)(struct vm_area_struct * area);
136     struct page * (*nopage)(struct vm_area_struct * area, 
                                unsigned long address, 
                                int unused);
137 };

The open() and close() functions are will be called every time a region is 
created or deleted. These functions are only used by a small number of devices, 
one filesystem and System V shared regions which need to perform additional 
operations when regions are opened or closed. For example, the System V open() 
callback will increment the number of VMAs using a shared segment (shp->shm_nattch).

The main operation of interest is the nopage() callback. This callback is used 
during a page fault by do_no_page(). The callback is responsible for locating 
the page in the page cache or allocating a page and populating it with the 
required data before returning it.


Most files that are mapped will use a generic vm_operations_struct() called 
generic_file_vm_ops. It registers only a nopage() function called filemap_nopage(). 
This nopage() function will either locating the page in the page cache or 
read the information from disk. The struct is declared as follows in mm/filemap.c:

2243 static struct vm_operations_struct generic_file_vm_ops = {
2244     nopage:         filemap_nopage,
2245 };



File/Device backed memory regions
In the event the region is backed by a file, the vm_file leads to an associated 
address_space. The struct contains information of relevance to the filesystem 
such as the number of dirty pages which must be flushed to disk. 

It is declared as follows in <linux/fs.h>:

406 struct address_space {
407     struct list_head        clean_pages;	//List of clean pages that need no synchronisation with backing stoarge    
408     struct list_head        dirty_pages;    //List of dirty pages that need synchronisation with backing storage
409     struct list_head        locked_pages;   //List of pages that are locked in memory
410     unsigned long           nrpages;        //Number of resident pages in use by the address space
411     struct address_space_operations *a_ops; //A struct of function for manipulating the filesystem.
412     struct inode            *host;          //The host inode the file belongs to
413     struct vm_area_struct   *i_mmap;        //A list of private mappings using this address_space
414     struct vm_area_struct   *i_mmap_shared; //A list of VMAs which share mappings in this address_space
415     spinlock_t              i_shared_lock;  //A spinlock to protect this structure
416     int                     gfp_mask;       //The mask to use when calling __alloc_pages() for new pages
417 };



Periodically the memory manager will need to flush information to disk. The
memory manager does not know and does not care how information is written to 
disk, so the a_ops struct is used to call the relevant functions.

385 struct address_space_operations {
386     int (*writepage)(struct page *);		//Write a page to disk.
387     int (*readpage)(struct file *, struct page *);	//Read a page from disk
388     int (*sync_page)(struct page *);		//Sync a dirty page with disk
389     /*
390      * ext3 requires that a successful prepare_write() call be
391      * followed by a commit_write() call - they must be balanced
392      */
393     int (*prepare_write)(struct file *, struct page *, 	//This is called before data is copied from userspace into 
                             unsigned, unsigned);		//a page that will be written to disk.
394     int (*commit_write)(struct file *, struct page *, 	//After the data has been copied from userspace, this function 
                             unsigned, unsigned);		//is called to commit the information to disk
395     /* Unfortunately this kludge is needed for FIBMAP. 
         * Don't use it */
396     int (*bmap)(struct address_space *, long);		//Maps a block so that raw IO can be performed
397     int (*flushpage) (struct page *, unsigned long);	//This makes sure there is no IO pending on a page before releasing it
398     int (*releasepage) (struct page *, int);		//This tries to flush all the buffers associated with a page 
								//before freeing the page itself
399 #define KERNEL_HAS_O_DIRECT
400     int (*direct_IO)(int, struct inode *, struct kiobuf *, 
                         unsigned long, int);
401 #define KERNEL_HAS_DIRECT_FILEIO
402     int (*direct_fileIO)(int, struct file *, struct kiobuf *, 
                             unsigned long, int);
403     void (*removepage)(struct page *);
404 };



Creating A Memory Region
The system call mmap() is provided for creating new memory regions within a process.


Finding a Mapped Memory Region
A common operation is to find the VMA a particular address belongs to, such 
as during operations like page faulting, and the function responsible for 
this is find_vma(). 

It first checks the mmap_cache field which caches the result of the last call 
to find_vma() as it is quite likely the same region will be needed a few times 
in succession. 

If it is not the desired region, the red-black tree stored in the mm_rb field 
is traversed. If the desired address is not contained within any VMA, the function 
will return the VMA closest to the requested address so it is important callers 
double check to ensure the returned VMA contains the desired address.

A second function called find_vma_prev() is provided which is functionally the 
same as find_vma() except that it also returns a pointer to the VMA preceding 
the desired VMA which is required as the list is a singly linked list. 

find_vma_prev() is rarely used but notably, it is used when two VMAs are being 
compared to determine if they may be merged. It is also used when removing a 
memory region so that the singly linked list may be updated.

The last function of note for searching VMAs is find_vma_intersection() which 
is used to find a VMA which overlaps a given address range. The most notable 
use of this is during a call to do_brk() when a region is growing up. It is 
important to ensure that the growing region will not overlap an old region.


Finding a Free Memory Region
When a new area is to be memory mapped, a free region has to be found that 
is large enough to contain the new mapping. The function responsible for 
finding a free area is get_unmapped_area()


Inserting a memory region
The principal function for inserting a new memory region is insert_vm_struct().
It is a very simple function which first calls find_vma_prepare() to find the 
appropriate VMAs the new region is to be inserted between and the correct nodes 
within the red-black tree. It then calls __vma_link() to do the work of 
linking in the new VMA.


Merging contiguous regions
Linux used to have a function called merge_segments() which was responsible for 
merging adjacent regions of memory together if the file and permissions matched.

The equivalent function which exists now is called vma_merge() and it is only 
used in two places. The first is user is sys_mmap() which calls it if an anonymous 
region is being mapped, as anonymous regions are frequently mergable. The second 
time is during do_brk() which is expanding one region into a newly allocated one 
where the two regions should be merged. Rather than merging two regions, the function 
vma_merge() checks if an existing region may be expanded to satisfy the new 
allocation negating the need to create a new region. A region may be expanded if 
there are no file or device mappings and the permissions of the two areas are the same.


Remapping and moving a memory region
mremap() is a system call provided to grow or shrink an existing memory mapping.
This is implemented by the function sys_mremap() which may move a memory region 
if it is growing or it would overlap another region and MREMAP_FIXED is not 
specified in the flags.

Locking a Memory Region
Linux can lock pages from an address range into memory via the system call mlock() 
which is implemented by sys_mlock(). At a high level, the function is simple; it 
creates a VMA for the address range to be locked, sets the VM_LOCKED flag on it 
and forces all the pages to be present with make_pages_present(). A second system 
call mlockall() which maps to sys_mlockall() is also provided which is a simple 
extension to do the same work as sys_mlock() except for every VMA on the calling 
process. Both functions rely on the core function do_mlock() to perform the real 
work of finding the affected VMAs and deciding what function is needed to fix up 
the regions as described later.

There are some limitations to what memory may be locked. The address range must 
be page aligned as VMAs are page aligned. This is addressed by simply rounding 
the range up to the nearest page aligned range. The second proviso is that the 
process limit RLIMIT_MLOCK imposed by the system administrator may not be exceeded. 
The last proviso is that each process may only lock half of physical memory at a 
time. This is a bit non-functional as there is nothing to stop a process forking 
a number of times and each child locking a portion but as only root processes are 
allowed to lock pages, it does not make much difference. It is safe to presume that 
a root process is trusted and knows what it is doing. If it does not, the system 
administrator with the resulting broken system probably deserves it and gets to 
keep both parts of it.





Deleting all memory regions
During process exit, it is necessary to unmap all VMAs associated with a mm_struct. 
The function responsible is exit_mmap().

It is a very simply function which flushes the CPU cache before walking through 
the linked list of VMAs, unmapping each of them in turn and freeing up the associated 
pages before flushing the TLB and deleting the page table entries.


Exception Handling
A very important part of VM is how kernel address space exceptions that are not 
bugs are caught. This section does not cover the exceptions that are raised with 
errors such as divide by zero, we are only concerned with the exception raised as 
the result of a page fault.

There are two situations where a bad reference may occur. The first is where a process 
sends an invalid pointer to the kernel via a system call which the kernel must be able 
to safely trap as the only check made initially is that the address is below PAGE_OFFSET. 
The second is where the kernel uses copy_from_user() or copy_to_user() to read or 
write data from userspace.

At compile time, the linker creates an exception table in the __ex_table section of 
the kernel code segment which starts at __start___ex_table and ends at __stop___ex_table. 
Each entry is of type exception_table_entry which is a pair consisting of an execution 
point and a fixup routine. When an exception occurs that the page fault handler cannot 
manage, it calls search_exception_table() to see if a fixup routine has been provided 
for an error at the faulting instruction. If module support is compiled, each modules 
exception table will also be searched.

If the address of the current exception is found in the table, the corresponding location 
of the fixup code is returned and executed.

Page Faulting
Pages in the process linear address space are not necessarily resident in memory. 
For example, allocations made on behalf of a process are not satisfied immediately 
as the space is just reserved within the vm_area_struct. Other examples of non-resident 
pages include the page having been swapped out to backing storage or writing a read-only page.

Exception				Type		Action
Region valid but page not allocated	Minor	Allocate a page frame from the physical page allocator
Region not valid but is beside an 	Minor	Expand the region and allocate a page
expandable region like the stack
Page swapped out but present in 	Minor	Re-establish the page in the process page tables and drop a reference to the swap cache
swap cache
Page swapped out to backing storage	Major	Find where the page with information stored in the PTE and read it from disk
Page write when marked read-only	Minor	If the page is a COW page, make a copy of it, mark it writable and assign it 
						to the process. If it is in fact a bad write, send a SIGSEGV signal
Region is invalid or process has	Error	Send a SEGSEGV signal to the process
no permissions to access
Fault occurred in the kernel 		Minor	If the fault occurred in the vmalloc area of the address space, the current 
portion address space				process page tables are updated against the master page table held by init_mm. 
						This is the only valid kernel page fault that may occur
Fault occurred in the userspace 	Error	If a fault occurs, it means a kernel system did not copy from userspace properly 
region while in kernel mode			and caused a page fault. This is a kernel bug which is treated quite severely.

					
					do_page_fault
				expand_stack	handle_mm_fault		force_sig_info	search_exception_table
		find_vma	find_vma_prev	pmd_alloc pte_alloc 	handle_pte_fault	search_one_table
								do_no_page do_swap_page do_wp_page establish_pte
						do_anonymous_page alloc_page lru_cache_add



Handling a Page Fault
Once the exception handler has decided the fault is a valid page fault in a valid 
memory region, the architecture-independent function handle_mm_fault(). It allocates 
the required page table entries if they do not already exist and calls handle_pte_fault().

The first stage of the decision is to check if the PTE is marked not present or if 
it has been allocated with which is checked by pte_present() and pte_none(). 
If no PTE has been allocated (pte_none() returned true), do_no_page() is called 
which handles Demand Allocation. Otherwise it is a page that has been swapped out 
to disk and do_swap_page() performs Demand Paging. There is a rare exception where 
swapped out pages belonging to a virtual file are handled by do_no_page(). 

The second option is if the page is being written to. If the PTE is write protected, 
then do_wp_page() is called as the page is a Copy-On-Write (COW) page. A COW page is 
one which is shared between multiple processes(usually a parent and child) until a 
write occurs after which a private copy is made for the writing process. A COW page 
is recognised because the VMA for the region is marked writable even though the 
individual PTE is not. If it is not a COW page, the page is simply marked dirty as 
it has been written to.

The last option is if the page has been read and is present but a fault still 
occurred. This can occur with some architectures that do not have a three level 
page table. In this case, the PTE is simply established and marked young.

Demand Allocation
When a process accesses a page for the very first time, the page has to be 
allocated and possibly filled with data by the do_no_page() function.
If the vm_operations_struct associated with the parent VMA (vma->vm_ops) 
provides a nopage() function, it is called. This is of importance to a 
memory mapped device such as a video card which needs to allocate the 
page and supply data on access or to a mapped file which must retrieve 
its data from backing storage. We will first discuss the case where the 
faulting page is anonymous as this is the simpliest case.

Handling anonymous pages
If vm_area_struct->vm_ops field is not filled or a nopage() function is 
not supplied, the function do_anonymous_page() is called to handle an 
anonymous access.

There are only two cases to handle, first time read and first time write.
As it is an anonymous page, the first read is an easy case as no data 
exists. In this case, the system-wide empty_zero_page, which is just a 
page of zeros, is mapped for the PTE and the PTE is write protected. 
The write protection is set so that another page fault will occur if 
the process writes to the page. On the x86, the global zero-filled page 
is zerod out in the function mem_init().

			do_no_page
			do_anonymous_page
	alloc_page	mark_page_accessed	lru_cache_add

If this is the first write to the page alloc_page() is called to allocate 
a free page and is zero filled by clear_user_highpage(). Assuming the page 
was successfully allocated, the Resident Set Size (RSS) field in the mm_struct 
will be incremented; flush_page_to_ram() is called as required when a page 
has been inserted into a userspace process by some architectures to ensure 
cache coherency. The page is then inserted on the LRU lists so it may be 
reclaimed later by the page reclaiming code. Finally the page table entries 
for the process are updated for the new mapping.

Handling file/device backed pages
If backed by a file or device, a nopage() function will be provided within 
the VMAs vm_operations_struct. In the file-backed case, the function 
filemap_nopage() is frequently the nopage() function for allocating a page 
and reading a page-sized amount of data from disk. 

Pages backed by a virtual file, such as those provided by shmfs, will use the 
function shmem_nopage(). Each device driver provides a different nopage() whose 
internals are unimportant to us here as long as it returns a valid struct page to use.

On return of the page, a check is made to ensure a page was successfully 
allocated and appropriate errors returned if not. A check is then made to see 
if an early COW break should take place. An early COW break will take place if 
the fault is a write to the page and the VM_SHARED flag is not included in the 
managing VMA. An early break is a case of allocating a new page and copying the 
data across before reducing the reference count to the page returned by the 
nopage() function.

In either case, a check is then made with pte_none() to ensure there is not a 
PTE already in the page table that is about to be used. It is possible with 
SMP that two faults would occur for the same page at close to the same time 
and as the spinlocks are not held for the full duration of the fault, this 
check has to be made at the last instant. If there has been no race, the PTE 
is assigned, statistics updated and the architecture hooks for cache coherency called.


Demand Paging
When a page is swapped out to backing storage, the function do_swap_page() is 
responsible for reading the page back in.

The information needed to find it is stored within the PTE itself. The 
information within the PTE is enough to find the page in swap. As pages may 
be shared between multiple processes, they can not always be swapped out 
immediately. Instead, when a page is swapped out, it is placed within the 
swap cache.


					do_swap_page
lookup_swap_cache swapfn_reachhead mark_page_accessed lock_page swap_free remove_exclusive_swap_page can_share_swap_page unlock_page
	 	  read_swap_cache_async  activate_page						     exclusive_swap_cache page
					activate_page_..

A shared page can not be swapped out immediately because there is no way of 
mapping a struct page to the PTEs of each process it is shared between. 
Searching the page tables of all processes is simply far too expensive. 

With the swap cache existing, it is possible that when a fault occurs it 
still exists in the swap cache. If it is, the reference count to the page 
is simply increased and it is placed within the process page tables again 
and registers as a minor page fault.

If the page exists only on disk swapin_readahead() is called which reads 
in the requested page and a number of pages after it. The number of pages 
read in is determined by the variable page_cluster defined in mm/swap.c. 
On low memory machines with less than 16MiB of RAM, it is initialised as 
2 or 3 otherwise. The number of pages read in is 2page_cluster unless a 
bad or empty swap entry is encountered. This works on the premise that a 
seek is the most expensive operation in time so once the seek has completed, 
the succeeding pages should also be read in.


Copy On Write (COW) Pages
Once upon time, the full parent address space was duplicated for a child when 
a process forked. This was an extremely expensive operation as it is possible 
a significant percentage of the process would have to be swapped in from 
backing storage. To avoid this considerable overhead, a technique called 
Copy-On-Write (COW) is employed.


During fork, the PTEs of the two processes are made read-only so that when a 
write occurs there will be a page fault. Linux recognises a COW page because 
even though the PTE is write protected, the controlling VMA shows the region 
is writable. It uses the function do_wp_page() to handle it by making a copy 
of the page and assigning it to the writing process. If necessary, a new swap 
slot will be reserved for the page. With this method, only the page table entries 
have to be copied during a fork.



Copying To/From Userspace
It is not safe to access memory in the process address space directly as there 
is no way to quickly check if the page addressed is resident or not. Linux relies 
on the MMU to raise exceptions when the address is invalid and have the Page Fault 
Exception handler catch the exception and fix it up. In the x86 case, assembler is 
provided by the __copy_user() to trap exceptions where the address is totally 
useless. The location of the fixup code is found when the function 
search_exception_table() is called. Linux provides an ample API (mainly macros) 
for copying data to and from the user address space safely

If the size of the copy is known at compile time, copy_from_user() calls 
__constant_copy_from_user() else __generic_copy_from_user() is used. If the size is 
known, there are different assembler optimisations to copy data in 1, 2 or 4 byte 
strides otherwise the distinction between the two copy functions is not important.

The generic copy function eventually calls the function __copy_user_zeroing() in 
<asm-i386/uaccess.h> which has three important parts. The first part is the 
assembler for the actual copying of size number of bytes from userspace. If any 
page is not resident, a page fault will occur and if the address is valid, it will 
get swapped in as normal. The second part is “fixup” code and the third part is 
the __ex_table mapping the instructions from the first part to the fixup code in 
the second part.

If an invalid address is read, the function do_page_fault() will fall through, 
call search_exception_table() and find the EIP where the faulty read took place 
and jump to the fixup code which copies zeros into the remaining kernel space, 
fixes up registers and returns. In this manner, the kernel can safely access 
userspace with no expensive checks and letting the MMU hardware handle the exceptions.
